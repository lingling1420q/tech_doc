都说后端自称是curd boy。

其实有1qps的curd, 1K qps 的curd，100K qps的curd。每个level的curd 都有不一样的挑战。

redis前文章提到的点都注意了优化了，但是在高并发下，还是会出现问题。

出个思考题:

假如搭建了一个redis集群，50个实例，然后按照上述优化，都尽量避免了一些高延迟的操作。

然后预估的每个实例2W qps的并发。这样理想情况下，集群能容纳50\*2= 100W的并发。

这个时候监控到20W 并发的时候，服务访问redis延迟越来大了。进而导致上游服务雪崩一样报警。并且严重的是加机器扩容都解决不了问题。

WHY？该注意的都注意了

最后排查，是因为一个热key，导致的。这个key 存储的是一个活动信息，推广后，大量的流量都会访问这个key。20W 流量至少10W都集中访问这个key的实例。导致了redis处理不过来。请求严重积压。

其实针对redis的热key，还是比较头疼的。如果是mysql有热点数据，加一层redis，就能解决。如果是redis，就没有什么可加了。特别是动态数据。

## 静态热key
这个倒是比较好接，如果是静态的，

最直接的是可以放到服务器配置中，直接服务处理。

如果是一批静态热key，不适合放程序。也可以将热key认为打散。例如，key+0到99 的随机数写到数据库。读的时候，key加上随机数去取就好了。


## 动态热key
比如像是热点事件这种，或者整点爆款商品这种，实时在变的，就比较复杂了。因为热点key其实是在动态变化的。

一种方案是程序配合（单实例）解决。
类似单实例做限流，程序定义一个热key的map，和key一个计数器，当计数器记录到达设定的某个阈值，例如每秒10次请求某个key，就加载到程序的map中。热key的map还需要设定上限，热key替换冷key，避免内存泄露。

这个缺点就是对程序改动比较大，对程序代码要求比较高。

第二个我想到的方案就是，方案一的逻辑，转移到redis的中间层代理上。有代理自动发现热key，并且处理。


## 备注
多啰嗦一句，即便是单机性能能处理热key，其实在高并发下，还涉及到网卡等问题，网卡被打满了，redis单机性能再强，还是会导致高延迟。
